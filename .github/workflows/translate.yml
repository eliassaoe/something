name: Translate Changed HTML Content to Spanish

on:
  push:
    branches: [ main ]
    paths:
      - '**.html'
  workflow_dispatch:

jobs:
  translate:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 2
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install openai beautifulsoup4 lxml
      
      - name: Extract, Translate and Replace Content of Changed Files Only
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          import os
          import openai
          from bs4 import BeautifulSoup, NavigableString, Comment
          import time
          import subprocess
          import re
          
          # Configure OpenAI API
          client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
          
          # Get list of HTML files changed in the most recent commit
          def get_changed_html_files():
              result = subprocess.run(
                  ['git', 'diff', '--name-only', 'HEAD^', 'HEAD'], 
                  capture_output=True, 
                  text=True
              )
              files = result.stdout.strip().split('\n')
              # Filter for only HTML files that exist
              return [f for f in files if f.endswith('.html') and os.path.exists(f)]
          
          def extract_translatable_content(soup):
              """Extract text content that should be translated - improved version."""
              elements_to_skip = ['script', 'style', 'noscript', 'iframe', 'canvas', 'svg', 'code', 'pre']
              translatable_nodes = []
              node_counter = 0
              url_slugs = set()  # To keep track of unique URL slugs
              
              # Extract text from title tag
              if soup.title and soup.title.string:
                  translatable_nodes.append({
                      'type': 'title',
                      'text': soup.title.string,
                      'node_id': 'title'
                  })
              
              # Extract text from meta tags (description, keywords)
              for meta in soup.find_all('meta'):
                  if meta.get('name') in ['description', 'keywords'] and meta.get('content'):
                      translatable_nodes.append({
                          'type': 'meta',
                          'text': meta['content'],
                          'node_id': f"meta_{meta['name']}"
                      })
                  elif meta.get('property') in ['og:title', 'og:description'] and meta.get('content'):
                      translatable_nodes.append({
                          'type': 'meta',
                          'text': meta['content'],
                          'node_id': f"meta_{meta['property'].replace(':', '_')}"
                      })
              
              # Extract canonical and og:url URLs if they exist
              canonical_link = soup.find('link', rel='canonical')
              if canonical_link and canonical_link.get('href'):
                  url = canonical_link['href']
                  # Extract the slug part (after the last /)
                  slug_match = re.search(r'/([^/]+)/?$', url)
                  if slug_match and not slug_match.group(1).endswith('.html'):
                      slug = slug_match.group(1)
                      translatable_nodes.append({
                          'type': 'canonical_slug',
                          'text': slug,
                          'node_id': 'canonical_slug',
                          'url': url
                      })
              
              og_url = soup.find('meta', property='og:url')
              if og_url and og_url.get('content'):
                  url = og_url['content']
                  # Extract the slug part (after the last /)
                  slug_match = re.search(r'/([^/]+)/?$', url)
                  if slug_match and not slug_match.group(1).endswith('.html'):
                      slug = slug_match.group(1)
                      translatable_nodes.append({
                          'type': 'og_url_slug',
                          'text': slug,
                          'node_id': 'og_url_slug',
                          'url': url
                      })
              
              # Find all internal links and extract slugs for translation
              for link in soup.find_all('a', href=True):
                  href = link['href']
                  # Skip external links, anchors, and javascript links
                  if (href.startswith('http') and 'unlimited-leads.online' in href) or \
                     (not href.startswith('http') and not href.startswith('#') and not href.startswith('javascript:')):
                      
                      # Extract the slug part (after the last /)
                      slug_match = re.search(r'/([^/]+)/?$', href)
                      if slug_match and not slug_match.group(1).endswith('.html'):
                          slug = slug_match.group(1)
                          # Skip if it's already in es folder
                          if not '/es/' in href and not slug.startswith('es/'):
                              if slug not in url_slugs:  # Only add unique slugs
                                  url_slugs.add(slug)
                                  node_counter += 1
                                  translatable_nodes.append({
                                      'type': 'url_slug',
                                      'text': slug,
                                      'node_id': f"url_slug_{node_counter}",
                                      'url': href,
                                      'element': link
                                  })
              
              # Specifically handle FAQ questions - they often have special formatting
              for faq_question in soup.select('.faq-question'):
                  # Get the text directly from the element, excluding children
                  question_text = ''.join(child for child in faq_question.contents if isinstance(child, NavigableString)).strip()
                  if question_text:
                      node_counter += 1
                      translatable_nodes.append({
                          'type': 'faq_question',
                          'text': question_text,
                          'node_id': f"faq_question_{node_counter}",
                          'element': faq_question
                      })
              
              # Handle all strong tags - they often contain important translatable content
              for strong in soup.find_all('strong'):
                  if strong.string and strong.string.strip():
                      node_counter += 1
                      translatable_nodes.append({
                          'type': 'strong',
                          'text': strong.string.strip(),
                          'node_id': f"strong_{node_counter}",
                          'element': strong
                      })
              
              # Handle list items specifically
              for li in soup.find_all('li'):
                  # First get direct text content excluding children
                  direct_text = ''.join(child for child in li.contents if isinstance(child, NavigableString)).strip()
                  if direct_text:
                      node_counter += 1
                      translatable_nodes.append({
                          'type': 'li_direct',
                          'text': direct_text,
                          'node_id': f"li_direct_{node_counter}",
                          'element': li
                      })
                  
                  # Also get text from HTML inside <li> tags that don't have children nodes
                  # This is for cases like <li><strong>Text</strong> - More text</li>
                  full_text = li.get_text().strip()
                  if full_text and not any(child.name for child in li.children if child.name):
                      node_counter += 1
                      translatable_nodes.append({
                          'type': 'li_full',
                          'text': full_text,
                          'node_id': f"li_full_{node_counter}",
                          'element': li
                      })
              
              # Function to recursively process all text nodes
              def process_node(node, path=''):
                  nonlocal node_counter
                  
                  # Skip certain elements
                  if node.name in elements_to_skip:
                      return
                  
                  # Skip comments
                  if isinstance(node, Comment):
                      return
                  
                  # For text nodes with content
                  if isinstance(node, NavigableString) and node.strip():
                      node_counter += 1
                      node_id = f"text_{node_counter}"
                      translatable_nodes.append({
                          'type': 'text',
                          'text': str(node).strip(),
                          'node_id': node_id,
                          'element': node.parent.name if node.parent else None,
                          'path': path
                      })
                  
                  # Process element nodes
                  elif hasattr(node, 'name') and node.name:
                      # Process attributes that should be translated
                      if node.name == 'img' and node.get('alt'):
                          node_counter += 1
                          translatable_nodes.append({
                              'type': 'attribute',
                              'text': node['alt'],
                              'node_id': f"img_alt_{node_counter}",
                              'element': node.name,
                              'attribute': 'alt',
                              'path': path
                          })
                      
                      if node.get('placeholder'):
                          node_counter += 1
                          translatable_nodes.append({
                              'type': 'attribute',
                              'text': node['placeholder'],
                              'node_id': f"placeholder_{node_counter}",
                              'element': node.name,
                              'attribute': 'placeholder',
                              'path': path
                          })
                      
                      if node.name == 'a' and node.get('title'):
                          node_counter += 1
                          translatable_nodes.append({
                              'type': 'attribute',
                              'text': node['title'],
                              'node_id': f"a_title_{node_counter}",
                              'element': node.name,
                              'attribute': 'title',
                              'path': path
                          })
                      
                      # Special handling for text in header, p, li, button, span, div elements
                      if node.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'button', 'span', 'div', 'a', 'label']:
                          # Check if this element has direct text (not just from children)
                          if node.string and node.string.strip():
                              node_counter += 1
                              node_id = f"{node.name}_{node_counter}"
                              translatable_nodes.append({
                                  'type': 'direct_text',
                                  'text': node.string.strip(),
                                  'node_id': node_id,
                                  'element': node.name,
                                  'path': path
                              })
                          # If this node contains a mix of text and other elements
                          elif any(isinstance(child, NavigableString) and child.strip() for child in node.children):
                              # Create a mapping of text nodes to their content
                              text_map = {}
                              for child in node.children:
                                  if isinstance(child, NavigableString) and child.strip():
                                      node_counter += 1
                                      text_id = f"mixed_{node.name}_{node_counter}"
                                      text_map[child] = text_id
                                      translatable_nodes.append({
                                          'type': 'mixed_text',
                                          'text': child.strip(),
                                          'node_id': text_id,
                                          'element': node.name,
                                          'path': path
                                      })
                      
                      # Recursively process all children
                      new_path = f"{path}/{node.name}" if path else node.name
                      for child in node.children:
                          process_node(child, new_path)
              
              # Start processing from the whole document
              process_node(soup)
              
              # Log some stats
              print(f"Extracted {len(translatable_nodes)} translatable items:")
              type_counts = {}
              for node in translatable_nodes:
                  type_counts[node['type']] = type_counts.get(node['type'], 0) + 1
              
              for type_name, count in type_counts.items():
                  print(f"  - {type_name}: {count} items")
              
              return translatable_nodes
          
          def translate_text_batch(texts, target_lang="Spanish"):
              """Translate a batch of texts."""
              if not texts:
                  return []
                  
              # Prepare the text for translation
              combined_text = "\n---ITEM---\n".join(texts)
              
              try:
                  response = client.chat.completions.create(
                      model="gpt-4",
                      messages=[
                          {"role": "system", "content": f"You are a professional translator specializing in website localization. Translate the following texts from French or English to {target_lang}. For URL slugs, create SEO-friendly translations that use hyphens instead of spaces and only contain lowercase letters, numbers, and hyphens. Maintain the same tone, style, and formatting for all other text. Return ONLY the translations, separated by '---ITEM---', in the same order as the input."},
                          {"role": "user", "content": combined_text}
                      ],
                      temperature=0.2
                  )
                  
                  # Split the translated text back into individual items
                  translation = response.choices[0].message.content
                  translated_texts = translation.split("\n---ITEM---\n")
                  
                  # Clean up any extra formatting
                  translated_texts = [text.strip() for text in translated_texts]
                  
                  # Make sure we got the same number of translations as inputs
                  if len(translated_texts) != len(texts):
                      print(f"Warning: Got {len(translated_texts)} translations for {len(texts)} inputs")
                      # Pad with empty strings if needed
                      if len(translated_texts) < len(texts):
                          translated_texts.extend([''] * (len(texts) - len(translated_texts)))
                      else:
                          translated_texts = translated_texts[:len(texts)]
                  
                  return translated_texts
                  
              except Exception as e:
                  print(f"Error in translation: {e}")
                  # If we hit rate limits, wait and retry
                  if "rate limit" in str(e).lower():
                      print("Rate limited, waiting 20 seconds...")
                      time.sleep(20)
                      return translate_text_batch(texts, target_lang)
                  return [''] * len(texts)
          
          def apply_translations(html_content, translatable_nodes, translations):
              """Apply translations back to the HTML content."""
              soup = BeautifulSoup(html_content, 'lxml')
              
              # Create a translation dictionary for easy lookup
              translation_dict = {node['node_id']: translation 
                                 for node, translation in zip(translatable_nodes, translations)
                                 if translation.strip()}
              
              # Create URL slug translation mapping
              url_slug_translations = {}
              for node, translation in zip(translatable_nodes, translations):
                  if node['type'] in ['url_slug', 'canonical_slug', 'og_url_slug'] and translation:
                      # Store both the original slug and its translation
                      original_slug = node['text']
                      translated_slug = translation.replace(' ', '-').lower()
                      url_slug_translations[original_slug] = translated_slug
              
              # Apply title translation
              if 'title' in translation_dict and soup.title:
                  soup.title.string = translation_dict['title']
              
              # Apply meta tag translations
              for meta in soup.find_all('meta'):
                  if meta.get('name') in ['description', 'keywords']:
                      node_id = f"meta_{meta['name']}"
                      if node_id in translation_dict:
                          meta['content'] = translation_dict[node_id]
                  elif meta.get('property') in ['og:title', 'og:description']:
                      node_id = f"meta_{meta['property'].replace(':', '_')}"
                      if node_id in translation_dict:
                          meta['content'] = translation_dict[node_id]
              
              # Update canonical and og:url URLs if they exist
              canonical_link = soup.find('link', rel='canonical')
              if canonical_link and canonical_link.get('href') and 'canonical_slug' in translation_dict:
                  original_url = canonical_link['href']
                  for original_slug, translated_slug in url_slug_translations.items():
                      # Replace the original slug with the translated one
                      if f"/{original_slug}" in original_url:
                          canonical_link['href'] = original_url.replace(f"/{original_slug}", f"/es/{translated_slug}")
                          break
              
              og_url = soup.find('meta', property='og:url')
              if og_url and og_url.get('content') and 'og_url_slug' in translation_dict:
                  original_url = og_url['content']
                  for original_slug, translated_slug in url_slug_translations.items():
                      # Replace the original slug with the translated one
                      if f"/{original_slug}" in original_url:
                          og_url['content'] = original_url.replace(f"/{original_slug}", f"/es/{translated_slug}")
                          break
              
              # Apply FAQ question translations
              for faq_question in soup.select('.faq-question'):
                  for node in translatable_nodes:
                      if node['type'] == 'faq_question' and 'element' in node and node['element'] == faq_question:
                          if node['node_id'] in translation_dict:
                              # We need to replace just the text content, not the icon
                              # First, get all children that are not NavigableString
                              non_text_children = [c for c in faq_question.children if not isinstance(c, NavigableString)]
                              # Clear the element
                              faq_question.clear()
                              # Add the translated text
                              faq_question.append(NavigableString(translation_dict[node['node_id']]))
                              # Add back the non-text children
                              for child in non_text_children:
                                  faq_question.append(child)
                              break
              
              # Apply strong tag translations
              for strong in soup.find_all('strong'):
                  if strong.string and strong.string.strip():
                      for node in translatable_nodes:
                          if node['type'] == 'strong' and node['text'] == strong.string.strip():
                              if node['node_id'] in translation_dict:
                                  strong.string = translation_dict[node['node_id']]
                                  break
              
              # Apply list item translations
              for li in soup.find_all('li'):
                  # Try to find matching li_full nodes first (for full element text)
                  full_text = li.get_text().strip()
                  for node in translatable_nodes:
                      if node['type'] == 'li_full' and node['text'] == full_text:
                          if node['node_id'] in translation_dict:
                              li.clear()
                              li.append(NavigableString(translation_dict[node['node_id']]))
                              break
                  
                  # Then try direct text (for text nodes without HTML)
                  direct_text = ''.join(child for child in li.contents if isinstance(child, NavigableString)).strip()
                  if direct_text:
                      for node in translatable_nodes:
                          if node['type'] == 'li_direct' and node['text'] == direct_text:
                              if node['node_id'] in translation_dict:
                                  # Replace only the direct text nodes
                                  for child in list(li.children):
                                      if isinstance(child, NavigableString):
                                          child.replace_with(NavigableString(''))
                                  # Add the translated text at the beginning
                                  li.insert(0, NavigableString(translation_dict[node['node_id']]))
                                  break
              
              # Update internal links with translated slugs
              for link in soup.find_all('a', href=True):
                  href = link['href']
                  
                  # Fix duplicate 'es/es' in links
                  if href.startswith('https://unlimited-leads.online/es/es'):
                      link['href'] = href.replace('/es/es', '/es')
                  
                  # Skip external links, anchors, and javascript links
                  if (href.startswith('http') and 'unlimited-leads.online' in href) or \
                     (not href.startswith('http') and not href.startswith('#') and not href.startswith('javascript:')):
                      
                      # Extract the slug part
                      slug_match = re.search(r'/([^/]+)/?$', href)
                      if slug_match and not slug_match.group(1).endswith('.html'):
                          original_slug = slug_match.group(1)
                          
                          # Skip if it's already in es folder
                          if not '/es/' in href and not original_slug.startswith('es/'):
                              # Look for translated version
                              if original_slug in url_slug_translations:
                                  translated_slug = url_slug_translations[original_slug]
                                  # Update the href
                                  if href.startswith('http'):
                                      # For absolute URLs
                                      new_href = href.replace(f"/{original_slug}", f"/es/{translated_slug}")
                                  else:
                                      # For relative URLs
                                      new_href = href.replace(f"{original_slug}", f"es/{translated_slug}")
                                  
                                  link['href'] = new_href
              
              # Create a mapping of paths for mixed content elements
              path_element_map = {}
              for node in translatable_nodes:
                  if 'path' in node and node['node_id'] in translation_dict:
                      if node['path'] not in path_element_map:
                          path_element_map[node['path']] = []
                      path_element_map[node['path']].append({
                          'node_id': node['node_id'],
                          'type': node['type'],
                          'translation': translation_dict[node['node_id']]
                      })
              
              # Function to recursively replace text
              def replace_text(element, path=''):
                  # Skip certain elements
                  if element.name in ['script', 'style', 'noscript', 'iframe', 'canvas', 'svg', 'code', 'pre']:
                      return
                  
                  # For text nodes
                  for child in list(element.children):
                      if isinstance(child, NavigableString) and child.strip():
                          # Look for matching translations
                          for node in translatable_nodes:
                              if (node['type'] == 'text' or node['type'] == 'mixed_text') and 'path' in node and node['path'] == path:
                                  if node['text'] == child.strip() and node['node_id'] in translation_dict:
                                      new_text = translation_dict[node['node_id']]
                                      if new_text:
                                          child.replace_with(NavigableString(new_text))
                                          break
                      elif hasattr(child, 'name') and child.name:
                          # Handle attributes
                          if child.name == 'img' and child.get('alt'):
                              for node in translatable_nodes:
                                  if node['type'] == 'attribute' and node['element'] == 'img' and node['attribute'] == 'alt':
                                      if node['text'] == child['alt'] and node['node_id'] in translation_dict:
                                          child['alt'] = translation_dict[node['node_id']]
                                          break
                          
                          if child.get('placeholder'):
                              for node in translatable_nodes:
                                  if node['type'] == 'attribute' and node['attribute'] == 'placeholder':
                                      if node['text'] == child['placeholder'] and node['node_id'] in translation_dict:
                                          child['placeholder'] = translation_dict[node['node_id']]
                                          break
                          
                          if child.name == 'a' and child.get('title'):
                              for node in translatable_nodes:
                                  if node['type'] == 'attribute' and node['element'] == 'a' and node['attribute'] == 'title':
                                      if node['text'] == child['title'] and node['node_id'] in translation_dict:
                                          child['title'] = translation_dict[node['node_id']]
                                          break
                          
                          # Special handling for text in header, p, li, etc.
                          if child.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'button', 'span', 'div', 'a', 'label']:
                              if child.string and child.string.strip():
                                  for node in translatable_nodes:
                                      if node['type'] == 'direct_text' and node['element'] == child.name:
                                          if node['text'] == child.string.strip() and node['node_id'] in translation_dict:
                                              child.string = translation_dict[node['node_id']]
                                              break
                          
                          # Recursively process children
                          new_path = f"{path}/{child.name}" if path else child.name
                          replace_text(child, new_path)
              
              # Start replacing from the document
              replace_text(soup)
              
              # Add language attribute to html tag
              if soup.html:
                  soup.html['lang'] = 'es'
              
              # Add hreflang tags
              head = soup.head or soup.new_tag('head')
              if not soup.head:
                  if soup.html:
                      soup.html.insert(0, head)
                  else:
                      soup.insert(0, soup.new_tag('html'))
                      soup.html.insert(0, head)
              
              # Remove existing hreflang tags
              for link in head.find_all('link', rel='alternate', href=True):
                  if 'hreflang' in link.attrs:
                      link.extract()
              
              # Add new hreflang tags
              # 1. Add hreflang for original language (en)
              en_link = soup.new_tag('link')
              en_link['rel'] = 'alternate'
              en_link['hreflang'] = 'en'
              base_url = "https://unlimited-leads.online"  # Replace with your actual domain
              if canonical_link and canonical_link.get('href'):
                  en_link['href'] = canonical_link['href']
              else:
                  # Construct from the current page
                  file_name = os.path.basename(file_path)
                  en_link['href'] = f"{base_url}/{file_name}"
              head.append(en_link)
              
              # 2. Add hreflang for Spanish
              es_link = soup.new_tag('link')
              es_link['rel'] = 'alternate'
              es_link['hreflang'] = 'es'
              if canonical_link and canonical_link.get('href'):
                  # Use the modified canonical URL
                  es_url = canonical_link['href']
                  for original_slug, translated_slug in url_slug_translations.items():
                      if f"/{original_slug}" in es_url:
                          es_url = es_url.replace(f"/{original_slug}", f"/es/{translated_slug}")
                          break
                  es_link['href'] = es_url
              else:
                  # Construct from the current page
                  file_name = os.path.basename(file_path)
                  es_link['href'] = f"{base_url}/es/{file_name}"
              head.append(es_link)
              
              return str(soup)
          
          # Get HTML files changed in the most recent commit
          changed_html_files = get_changed_html_files()
          print(f"Found {len(changed_html_files)} changed HTML files")
          
          for file_path in changed_html_files:
              # Skip files that are already in language folders
              if '/es/' in file_path or file_path.startswith('es/'):
                  print(f"Skipping {file_path} - already a translation")
                  continue
                  
              print(f"Processing {file_path}")
              
              try:
                  # Read HTML file
                  with open(file_path, 'r', encoding='utf-8') as f:
                      html_content = f.read()
                  
                  # Parse HTML
                  soup = BeautifulSoup(html_content, 'lxml')
                  
                  # Extract translatable content
                  print(f"Extracting translatable content from {file_path}...")
                  translatable_nodes = extract_translatable_content(soup)
                  print(f"Found {len(translatable_nodes)} translatable items")
                  
                  if not translatable_nodes:
                      print(f"No translatable content found in {file_path}, skipping")
                      continue
                  
                  # Batch translations (process in chunks of 50 items to avoid token limits)
                  all_translations = []
                  batch_size = 50
                  
                  for i in range(0, len(translatable_nodes), batch_size):
                      batch = translatable_nodes[i:i+batch_size]
                      texts_to_translate = [node['text'] for node in batch]
                      
                      print(f"Translating batch {i//batch_size + 1}/{(len(translatable_nodes) + batch_size - 1)//batch_size}...")
                      translations = translate_text_batch(texts_to_translate)
                      all_translations.extend(translations)
                      
                      # Add a delay between batches to avoid rate limits
                      # Add a delay between batches to avoid rate limits
                      if i + batch_size < len(translatable_nodes):
                          time.sleep(5)
                  
                  # Apply translations back to HTML
                  print(f"Applying translations to {file_path}...")
                  translated_html = apply_translations(html_content, translatable_nodes, all_translations)
                  
                  # Create output directory structure
                  dir_name = os.path.dirname(file_path)
                  base_name = os.path.basename(file_path)
                  output_dir = os.path.join(dir_name, 'es') if dir_name else 'es'
                  os.makedirs(output_dir, exist_ok=True)
                  
                  # Save translated file
                  output_path = os.path.join(output_dir, base_name)
                  with open(output_path, 'w', encoding='utf-8') as f:
                      f.write(translated_html)
                      
                  print(f"Translation saved to {output_path}")
                  
              except Exception as e:
                  print(f"Error processing {file_path}: {e}")
        shell: python
      
      - name: Commit changes
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add --all
          git diff --staged --quiet || git commit -m "Add Spanish translations for changed files"
          git push
